{"guid":"af0fccee-8f17-4d24-ba98-f4d46f72f4ce","title":"Configure access to Hadoop","path":"help\\installation\\using\\configure-fda-hadoop.md","fullText":"---\nproduct: campaign\ntitle: Configure access to Hadoop\ndescription: Learn how to configure access to Hadoop in FDA\nfeature: Installation, Federated Data Access\naudience: platform\ncontent-type: reference\ntopic-tags: connectors\nexl-id: e3a97e55-dd8b-41e1-b48c-816d973f62a8\n---\n# Configure access to Hadoop {#configure-access-to-hadoop}\n\n\n\nUse Campaign **Federated Data Access** (FDA) option to process information stored in an external databases. Follow the steps below to configure access to Hadoop.\n\n1. Configure [Hadoop database](#configuring-hadoop) \n1. Configure the Hadoop [external account](#hadoop-external) in Campaign\n\n## Configuring Hadoop 3.0 {#configuring-hadoop}\n\nConnecting to a Hadoop external database in FDA requires the following configurations on the Adobe Campaign server. Note that this configuration is available for both Windows and Linux.\n\n1. Download the ODBC drivers for Hadoop depending on your OS version. Drivers can be found on [this page](https://www.cloudera.com/downloads.html).\n\n1. You then need to install the ODBC drivers and create a DSN for your Hive connection. Instructions can be found in [this page](https://docs.cloudera.com/documentation/other/connectors/hive-odbc/2-6-5/Cloudera-ODBC-Driver-for-Apache-Hive-Install-Guide.pdf)\n\n1. After downloading and installing the ODBC drivers, you need to restart Campaign Classic. To do so, run the following command:\n\n     ```\n     systemctl stop nlserver.service\n     systemctl start nlserver.service\n     ```\n\n1. In Campaign Classic, you can then configure your [!DNL Hadoop] external account. For more on how to configure your external account, refer to [this section](#hadoop-external).\n\n## Hadoop external account {#hadoop-external}\n\nThe [!DNL Hadoop] external account allows you to connect your Campaign instance to your Hadoop external database.\n\n1. In Campaign Classic, configure your [!DNL Hadoop] external account. From the **[!UICONTROL Explorer]**, click **[!UICONTROL Administration]** / **[!UICONTROL Platform]** / **[!UICONTROL External accounts]**.\n\n1. Click **[!UICONTROL New]**.\n\n1. Select **[!UICONTROL External database]** as your external account's **[!UICONTROL Type]**.\n\n1. Configure the **[!UICONTROL Hadoop]** external account, you must specify:\n\n    * **[!UICONTROL Type]**: ODBC (Sybase ASE, Sybase IQ)\n\n    * **[!UICONTROL Server]**: Name of the DNS\n\n    * **[!UICONTROL Account]**: Name of the user\n\n    * **[!UICONTROL Password]**: User account password\n\n    * **[!UICONTROL Database]**: Name of your database if not specified in DSN. It can be left empty if specified in the DSN\n\n    * **[!UICONTROL Time zone]**: Server time zone\n\n    ![](assets/hadoop3.png)\n\nThe connector supports the following ODBC options:\n\n| Name |  Value |\n|---|---|\n|  ODBCMgr | iODBC |\n|  warehouse |  1/2/4 |\n\nThe connector also supports the following Hive options:\n\n| Name |  Value |  Description |\n|---|---|---|\n|  bulkKey |  Azure blob or DataLake access key |  For wasb:// or wasbs:// bulk loaders (i.e. if the bulk load tool starts with wasb:// or wasbs://). <br>It is the access key for blob or DataLake bucket for bulk load. |\n|  hdfsPort |  port number <br>set by default to 8020 |  For HDFS bulk load (i.e. if the bulk load tool starts with webhdfs:// or webhdfss://). |\n|  bucketsNumber |  20 |  Number of buckets when creating a clustered table. |\n|  fileFormat |  PARQUET |  Default file format for work tables. |\n\n\n## Configuring Hadoop 2.1 {#configure-access-hadoop-2}\n\nIf you need to connect to Hadoop 2.1, follow the steps described below for [Windows](#for-windows) or [Linux](#for-linux).\n\n### Hadoop 2.1 for Windows {#for-windows}\n\n1. Install ODBC and [Azure HD Insight](https://www.microsoft.com/en-us/download/details.aspx?id=40886) drivers for Windows.\n1. Create the DSN (Data Source Name) by running the ODBC DataSource Administrator tool. A System DSN sample for Hive is provided for you to modify.\n\n   ```\n   Description: vorac (or any name you like)\n   Host: vorac.azurehdinsight.net\n   Port: 443\n   Database: sm_tst611 (or your database name)\n   Mechanism: Azure HDInsight Service\n   User/Password: admin/<your password here>\n   ```\n\n1. Create the Hadoop external account, as detailed in [this section](#hadoop-external).\n\n### Hadoop 2.1 for Linux {#for-linux}\n\n1. Install unixodbc for Linux.\n\n   ```\n   apt-get install unixodbc\n   ```\n\n1. Download and install ODBC drivers for Apache Hive from HortonWorks: [https://www.cloudera.com/downloads.html](https://www.cloudera.com/downloads.html).\n\n   ```\n   dpkg -i hive-odbc-native_2.1.10.1014-2_amd64.deb\n   ```\n\n1. Check ODBC files location.\n\n   ```\n\n   root@campadpac71:/tmp# odbcinst -j\n   unixODBC 2.3.1\n   DRIVERS............: /etc/odbcinst.ini\n   SYSTEM DATA SOURCES: /etc/odbc.ini\n   FILE DATA SOURCES..: /etc/ODBCDataSources\n   USER DATA SOURCES..: /root/.odbc.ini\n   SQLULEN Size.......: 8\n   SQLLEN Size........: 8\n   SQLSETPOSIROW Size.: 8\n   ```\n\n1. Create the DSN (Data Source Name) and edit the odbc.ini file. Then, create a DSN for your Hive connection.\n\n   Here is an example for HDInsight to setup a connection called \"viral\":\n\n   ```\n   [ODBC Data Sources]\n   vorac \n\n   [vorac]\n   Driver=/usr/lib/hive/lib/native/Linux-amd64-64/libhortonworkshiveodbc64.so\n   HOST=vorac.azurehdinsight.net\n   PORT=443\n   Schema=sm_tst611\n   HiveServerType=2\n   AuthMech=6\n   UID=admin\n   PWD=<your password here>\n   HTTPPath=\n   UseNativeQuery=1\n   ```\n\n   >[!NOTE]\n   >\n   >The **UseNativeQuery** parameter here is very important. Campaign is Hive-aware and will not work correctly unless UseNativeQuery is set. Typically, the driver or Hive SQL Connector will rewrite queries and tamper the column ordering.\n\n   The authentication setup depends on the Hive/Hadoop configuration. For instance, for HD Insight, use AuthMech=6 for user/password authentication, as described [here](https://www.simba.com/products/Spark/doc/ODBC_InstallGuide/unix/content/odbc/hi/configuring/authenticating/azuresvc.htm).\n\n1. Export the variables.\n\n   ```\n   export ODBCINI=/etc/myodbc.ini\n   export ODBCSYSINI=/etc/myodbcinst.ini\n   ```\n\n1. Setup Hortonworks drivers via /usr/lib/hive/lib/native/Linux-amd64-64/hortonworks.hiveodbc.ini.\n\n   You have to use UTF-16 to be able to connect with Campaign and unix-odbc (libodbcinst).\n\n   ```\n   [Driver]\n\n   DriverManagerEncoding=UTF-16\n   ErrorMessagesPath=/usr/lib/hive/lib/native/hiveodbc/ErrorMessages/\n   LogLevel=0\n   LogPath=/tmp/hive\n   SwapFilePath=/tmp\n\n   ODBCInstLib=libodbcinst.so\n   ```\n\n1. You can now test your connection using isql.\n\n   ```\n   isql vorac\n   isql vorac -v\n   ```\n\n1. Create the Hadoop external account, as detailed in [this section](#hadoop-external).\n","headers":[["title","Configure access to Hadoop"],["description","Learn how to configure access to Hadoop in FDA"],["feature","Installation, Federated Data Access"],["topic-tags","connectors"]],"sections":[{"section":"Configure access to Hadoop","sectionId":"80667663-732a-4f3c-a7b5-6e40eabcf6b1","paragraphs":["Use Campaign Federated Data Access (FDA) option to process information stored in an external databases. Follow the steps below to configure access to Hadoop.","Configure Hadoop database \nConfigure the Hadoop external account in Campaign"]},{"section":"Configuring Hadoop 3.0","sectionId":"35609131-5bdb-4705-b274-fe8de09558d2","paragraphs":["Connecting to a Hadoop external database in FDA requires the following configurations on the Adobe Campaign server. Note that this configuration is available for both Windows and Linux.","Download the ODBC drivers for Hadoop depending on your OS version. Drivers can be found on this page.","You then need to install the ODBC drivers and create a DSN for your Hive connection. Instructions can be found in this page","After downloading and installing the ODBC drivers, you need to restart Campaign Classic. To do so, run the following command:","systemctl stop nlserver.service\nsystemctl start nlserver.service","In Campaign Classic, you can then configure your DNL Hadoop external account. For more on how to configure your external account, refer to this section."]},{"section":"Hadoop external account","sectionId":"155f1b6f-f4e5-47b0-8dfb-df0622056b36","paragraphs":["The DNL Hadoop external account allows you to connect your Campaign instance to your Hadoop external database.","In Campaign Classic, configure your DNL Hadoop external account. From the Explorer, click Administration / Platform / External accounts.","Click New.","Select External database as your external account's Type.","Configure the Hadoop external account, you must specify:","Type: ODBC (Sybase ASE, Sybase IQ)","Server: Name of the DNS","Account: Name of the user","Password: User account password","Database: Name of your database if not specified in DSN. It can be left empty if specified in the DSN","Time zone: Server time zone","The connector supports the following ODBC options:","Name Value\nODBCMgr iODBC\nwarehouse 1/2/4","The connector also supports the following Hive options:","Name Value Description\nbulkKey Azure blob or DataLake access key For wasb:// or wasbs:// bulk loaders (i.e. if the bulk load tool starts with wasb:// or wasbs://). It is the access key for blob or DataLake bucket for bulk load.\nhdfsPort port number set by default to 8020 For HDFS bulk load (i.e. if the bulk load tool starts with webhdfs:// or webhdfss://).\nbucketsNumber 20 Number of buckets when creating a clustered table.\nfileFormat PARQUET Default file format for work tables."]},{"section":"Configuring Hadoop 2.1","sectionId":"8c3f3fbf-ebae-418c-b0e9-99f64ebb2f7e","paragraphs":["If you need to connect to Hadoop 2.1, follow the steps described below for Windows or Linux."]},{"section":"Hadoop 2.1 for Windows","sectionId":"19d56578-789f-4a96-bd17-9695640428f8","paragraphs":["Install ODBC and Azure HD Insight drivers for Windows.","Create the DSN (Data Source Name) by running the ODBC DataSource Administrator tool. A System DSN sample for Hive is provided for you to modify.","Description: vorac (or any name you like)\nHost: vorac.azurehdinsight.net\nPort: 443\nDatabase: sm_tst611 (or your database name)\nMechanism: Azure HDInsight Service\nUser/Password: admin/<your password here>","Create the Hadoop external account, as detailed in this section."]},{"section":"Hadoop 2.1 for Linux","sectionId":"2ef8c916-496a-4400-a50b-e68b9f0d96c5","paragraphs":["Install unixodbc for Linux.","apt-get install unixodbc","Download and install ODBC drivers for Apache Hive from HortonWorks: https://www.cloudera.com/downloads.html.","dpkg -i hive-odbc-native_2.1.10.1014-2_amd64.deb","Check ODBC files location.","root@campadpac71:/tmp# odbcinst -j\nunixODBC 2.3.1\nDRIVERS............: /etc/odbcinst.ini\nSYSTEM DATA SOURCES: /etc/odbc.ini\nFILE DATA SOURCES..: /etc/ODBCDataSources\nUSER DATA SOURCES..: /root/.odbc.ini\nSQLULEN Size.......: 8\nSQLLEN Size........: 8\nSQLSETPOSIROW Size.: 8","Create the DSN (Data Source Name) and edit the odbc.ini file. Then, create a DSN for your Hive connection.","Here is an example for HDInsight to setup a connection called \"viral\":","[ODBC Data Sources]\nvorac","[vorac]\nDriver=/usr/lib/hive/lib/native/Linux-amd64-64/libhortonworkshiveodbc64.so\nHOST=vorac.azurehdinsight.net\nPORT=443\nSchema=sm_tst611\nHiveServerType=2\nAuthMech=6\nUID=admin\nPWD=<your password here>\nHTTPPath=\nUseNativeQuery=1","NOTE","The UseNativeQuery parameter here is very important. Campaign is Hive-aware and will not work correctly unless UseNativeQuery is set. Typically, the driver or Hive SQL Connector will rewrite queries and tamper the column ordering.","The authentication setup depends on the Hive/Hadoop configuration. For instance, for HD Insight, use AuthMech=6 for user/password authentication, as described here.","Export the variables.","export ODBCINI=/etc/myodbc.ini\nexport ODBCSYSINI=/etc/myodbcinst.ini","Setup Hortonworks drivers via /usr/lib/hive/lib/native/Linux-amd64-64/hortonworks.hiveodbc.ini.","You have to use UTF-16 to be able to connect with Campaign and unix-odbc (libodbcinst).","[Driver]","DriverManagerEncoding=UTF-16\nErrorMessagesPath=/usr/lib/hive/lib/native/hiveodbc/ErrorMessages/\nLogLevel=0\nLogPath=/tmp/hive\nSwapFilePath=/tmp","ODBCInstLib=libodbcinst.so","You can now test your connection using isql.","isql vorac\nisql vorac -v"]},{"section":"odbcinst -j","sectionId":"4e6736ad-905f-470a-959a-713e6b32e2d3","paragraphs":["Configure access to Hadoop","Use Campaign Federated Data Access (FDA) option to process information stored in an external databases. Follow the steps below to configure access to Hadoop.","Configure Hadoop database \nConfigure the Hadoop external account in Campaign","Configuring Hadoop 3.0","Connecting to a Hadoop external database in FDA requires the following configurations on the Adobe Campaign server. Note that this configuration is available for both Windows and Linux.","Download the ODBC drivers for Hadoop depending on your OS version. Drivers can be found on this page.","You then need to install the ODBC drivers and create a DSN for your Hive connection. Instructions can be found in this page","After downloading and installing the ODBC drivers, you need to restart Campaign Classic. To do so, run the following command:","systemctl stop nlserver.service\nsystemctl start nlserver.service","In Campaign Classic, you can then configure your DNL Hadoop external account. For more on how to configure your external account, refer to this section.","Hadoop external account","The DNL Hadoop external account allows you to connect your Campaign instance to your Hadoop external database.","In Campaign Classic, configure your DNL Hadoop external account. From the Explorer, click Administration / Platform / External accounts.","Click New.","Select External database as your external account's Type.","Configure the Hadoop external account, you must specify:","Type: ODBC (Sybase ASE, Sybase IQ)","Server: Name of the DNS","Account: Name of the user","Password: User account password","Database: Name of your database if not specified in DSN. It can be left empty if specified in the DSN","Time zone: Server time zone","The connector supports the following ODBC options:","Name Value\nODBCMgr iODBC\nwarehouse 1/2/4","The connector also supports the following Hive options:","Name Value Description\nbulkKey Azure blob or DataLake access key For wasb:// or wasbs:// bulk loaders (i.e. if the bulk load tool starts with wasb:// or wasbs://). It is the access key for blob or DataLake bucket for bulk load.\nhdfsPort port number set by default to 8020 For HDFS bulk load (i.e. if the bulk load tool starts with webhdfs:// or webhdfss://).\nbucketsNumber 20 Number of buckets when creating a clustered table.\nfileFormat PARQUET Default file format for work tables.","Configuring Hadoop 2.1","If you need to connect to Hadoop 2.1, follow the steps described below for Windows or Linux.","Hadoop 2.1 for Windows","Install ODBC and Azure HD Insight drivers for Windows.","Create the DSN (Data Source Name) by running the ODBC DataSource Administrator tool. A System DSN sample for Hive is provided for you to modify.","Description: vorac (or any name you like)\nHost: vorac.azurehdinsight.net\nPort: 443\nDatabase: sm_tst611 (or your database name)\nMechanism: Azure HDInsight Service\nUser/Password: admin/<your password here>","Create the Hadoop external account, as detailed in this section.","Hadoop 2.1 for Linux","Install unixodbc for Linux.","apt-get install unixodbc","Download and install ODBC drivers for Apache Hive from HortonWorks: https://www.cloudera.com/downloads.html.","dpkg -i hive-odbc-native_2.1.10.1014-2_amd64.deb","Check ODBC files location.","root@campadpac71:/tmp# odbcinst -j\nunixODBC 2.3.1\nDRIVERS............: /etc/odbcinst.ini\nSYSTEM DATA SOURCES: /etc/odbc.ini\nFILE DATA SOURCES..: /etc/ODBCDataSources\nUSER DATA SOURCES..: /root/.odbc.ini\nSQLULEN Size.......: 8\nSQLLEN Size........: 8\nSQLSETPOSIROW Size.: 8","Create the DSN (Data Source Name) and edit the odbc.ini file. Then, create a DSN for your Hive connection.","Here is an example for HDInsight to setup a connection called \"viral\":","[ODBC Data Sources]\nvorac","[vorac]\nDriver=/usr/lib/hive/lib/native/Linux-amd64-64/libhortonworkshiveodbc64.so\nHOST=vorac.azurehdinsight.net\nPORT=443\nSchema=sm_tst611\nHiveServerType=2\nAuthMech=6\nUID=admin\nPWD=<your password here>\nHTTPPath=\nUseNativeQuery=1","NOTE","The UseNativeQuery parameter here is very important. Campaign is Hive-aware and will not work correctly unless UseNativeQuery is set. Typically, the driver or Hive SQL Connector will rewrite queries and tamper the column ordering.","The authentication setup depends on the Hive/Hadoop configuration. For instance, for HD Insight, use AuthMech=6 for user/password authentication, as described here.","Export the variables.","export ODBCINI=/etc/myodbc.ini\nexport ODBCSYSINI=/etc/myodbcinst.ini","Setup Hortonworks drivers via /usr/lib/hive/lib/native/Linux-amd64-64/hortonworks.hiveodbc.ini.","You have to use UTF-16 to be able to connect with Campaign and unix-odbc (libodbcinst).","[Driver]","DriverManagerEncoding=UTF-16\nErrorMessagesPath=/usr/lib/hive/lib/native/hiveodbc/ErrorMessages/\nLogLevel=0\nLogPath=/tmp/hive\nSwapFilePath=/tmp","ODBCInstLib=libodbcinst.so","You can now test your connection using isql.","isql vorac\nisql vorac -v","Create the Hadoop external account, as detailed in this section."]}]}